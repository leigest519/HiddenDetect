<div align="center">
<h1>HiddenDetect: Detecting Jailbreak Attacks against Multimodal Large Language Models via Monitoring Hidden States</h1>


[[ Arxiv ]](https://arxiv.org/abs/2502.14744v1) [[ Huggingface Daily Paper ]](https://huggingface.co/papers/2502.14744) 

</div>

## Introduction
In this work , we investigate whether LVLMs inherently encode safety-relevant signals within their internal activations during inference. Our findings reveal that LVLMs exhibit distinct activation patterns when processing unsafe prompts, which can be leveraged to detect and mitigate adversarial inputs without requiring extensive fine-tuning. Building on this insight, we introduce HiddenDetect, a novel tuning-free framework that harnesses internal model activations to enhance safety.

## News
- **2025.02.21** Build README file. Will release the code and dataset in the upcoming week.

## License
HiddenDetect is released under the [Apache License 2.0](LICENSE)

## Citation
If you find our models / code / papers useful in your research, please consider giving ‚≠ê and citations üìù, thx :)  
```bibtex
@misc{jiang2025hiddendetectdetectingjailbreakattacks,
      title={HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States}, 
      author={Yilei Jiang and Xinyan Gao and Tianshuo Peng and Yingshui Tan and Xiaoyong Zhu and Bo Zheng and Xiangyu Yue},
      year={2025},
      eprint={2502.14744},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.14744}, 
}
```

## Contact Us
If you encounter any issues or have questions, please feel free to contact us via yljiang@link.cuhk.edu.hk.
